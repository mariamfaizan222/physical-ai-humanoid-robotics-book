{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/physical-ai-book/docs","tagsPath":"/physical-ai-book/docs/tags","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\HP\\Desktop\\book-5\\sidebars.js","contentPath":"C:\\Users\\HP\\Desktop\\book-5\\docs","docs":[{"id":"intro","title":"Introduction","description":"Welcome to \"Building Intelligent Humanoid Robots: From Simulation to Reality\"! This comprehensive guide is designed for students, robotics hobbyists, and AI developers eager to dive into the exciting world of humanoid robotics.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/physical-ai-book/docs/intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction","sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Chapter 1: Introduction to Physical AI","permalink":"/physical-ai-book/docs/module-1/chapter-1"}},{"id":"Introduction","title":"Introduction","description":"The quest to build machines that can understand, interact with, and navigate our world with human-like dexterity and intelligence has long been a frontier of scientific and engineering ambition. Today, the convergence of advanced robotics, sophisticated artificial intelligence, and powerful simulation technologies is bringing this vision closer to reality. This book, \"Building Intelligent Humanoid Robots: From Simulation to Reality,\" is your comprehensive guide to this exciting interdisciplinary field.","source":"@site/docs/Introduction.md","sourceDirName":".","slug":"/Introduction","permalink":"/physical-ai-book/docs/Introduction","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction","sidebar_position":1}},{"id":"module-1/chapter-1","title":"Chapter 1: Introduction to Physical AI","description":"What is Physical AI?","source":"@site/docs/module-1/chapter-1.md","sourceDirName":"module-1","slug":"/module-1/chapter-1","permalink":"/physical-ai-book/docs/module-1/chapter-1","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/physical-ai-book/docs/intro"},"next":{"title":"Chapter 2: Sensor Systems","permalink":"/physical-ai-book/docs/module-1/chapter-2"}},{"id":"module-1/chapter-2","title":"Chapter 2: Sensor Systems","description":"Introduction to Robot Perception","source":"@site/docs/module-1/chapter-2.md","sourceDirName":"module-1","slug":"/module-1/chapter-2","permalink":"/physical-ai-book/docs/module-1/chapter-2","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Physical AI","permalink":"/physical-ai-book/docs/module-1/chapter-1"},"next":{"title":"Chapter 3: ROS 2 Architecture","permalink":"/physical-ai-book/docs/module-1/chapter-3"}},{"id":"module-1/chapter-3","title":"Chapter 3: ROS 2 Architecture","description":"What is ROS 2?","source":"@site/docs/module-1/chapter-3.md","sourceDirName":"module-1","slug":"/module-1/chapter-3","permalink":"/physical-ai-book/docs/module-1/chapter-3","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Sensor Systems","permalink":"/physical-ai-book/docs/module-1/chapter-2"},"next":{"title":"ROS 2 Packages and Workspaces","permalink":"/physical-ai-book/docs/module-1/chapter-4"}},{"id":"module-1/chapter-4","title":"ROS 2 Packages and Workspaces","description":"In ROS 2, the fundamental unit of code organization and distribution is the package. Packages encapsulate related code, configuration files, launch files, and other resources. Understanding how to create, manage, and build these packages, along with the concept of workspaces, is crucial for any ROS 2 developer.","source":"@site/docs/module-1/chapter-4.md","sourceDirName":"module-1","slug":"/module-1/chapter-4","permalink":"/physical-ai-book/docs/module-1/chapter-4","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"ROS 2 Packages and Workspaces","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: ROS 2 Architecture","permalink":"/physical-ai-book/docs/module-1/chapter-3"},"next":{"title":"ROS 2 Communication Patterns and Best Practices","permalink":"/physical-ai-book/docs/module-1/chapter-5"}},{"id":"module-1/chapter-5","title":"ROS 2 Communication Patterns and Best Practices","description":"Effective communication between different parts of a robotic system is fundamental. ROS 2 provides several powerful communication mechanisms, each suited for different types of data exchange. Understanding these patterns—Topics, Services, and Actions—and employing best practices will lead to more robust, efficient, and maintainable robotic software.","source":"@site/docs/module-1/chapter-5.md","sourceDirName":"module-1","slug":"/module-1/chapter-5","permalink":"/physical-ai-book/docs/module-1/chapter-5","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"ROS 2 Communication Patterns and Best Practices","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Packages and Workspaces","permalink":"/physical-ai-book/docs/module-1/chapter-4"},"next":{"title":"Introduction to Gazebo Simulation for Robotics","permalink":"/physical-ai-book/docs/module-2/chapter-6"}},{"id":"module-2/chapter-6","title":"Introduction to Gazebo Simulation for Robotics","description":"Simulation is an indispensable tool in modern robotics development. It allows us to test algorithms, design controllers, and train AI models in a safe, cost-effective, and repeatable virtual environment before deploying them on physical hardware. Gazebo is a powerful, open-source 3D robotics simulator that is widely used in both research and industry.","source":"@site/docs/module-2/chapter-6.md","sourceDirName":"module-2","slug":"/module-2/chapter-6","permalink":"/physical-ai-book/docs/module-2/chapter-6","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Introduction to Gazebo Simulation for Robotics","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Communication Patterns and Best Practices","permalink":"/physical-ai-book/docs/module-1/chapter-5"},"next":{"title":"URDF Modeling for Humanoid Robots","permalink":"/physical-ai-book/docs/module-2/chapter-7"}},{"id":"module-2/chapter-7","title":"URDF Modeling for Humanoid Robots","description":"To simulate and control robots, especially complex ones like humanoids, we need a way to describe their physical structure, including their components, how they are connected, and their physical properties. The Unified Robot Description Format (URDF) is an XML-based file format used by ROS to represent a robot's kinematics and dynamics.","source":"@site/docs/module-2/chapter-7.md","sourceDirName":"module-2","slug":"/module-2/chapter-7","permalink":"/physical-ai-book/docs/module-2/chapter-7","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"URDF Modeling for Humanoid Robots","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Gazebo Simulation for Robotics","permalink":"/physical-ai-book/docs/module-2/chapter-6"},"next":{"title":"Simulating Robot Sensors in Gazebo","permalink":"/physical-ai-book/docs/module-2/chapter-8"}},{"id":"module-2/chapter-8","title":"Simulating Robot Sensors in Gazebo","description":"Real robots rely on a suite of sensors to perceive their environment and determine their own state. In simulation, accurately modeling these sensors is crucial for developing and testing algorithms that will eventually run on physical hardware. Gazebo provides robust support for simulating various sensors, allowing us to generate realistic data that mimics real-world conditions.","source":"@site/docs/module-2/chapter-8.md","sourceDirName":"module-2","slug":"/module-2/chapter-8","permalink":"/physical-ai-book/docs/module-2/chapter-8","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Simulating Robot Sensors in Gazebo","sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"URDF Modeling for Humanoid Robots","permalink":"/physical-ai-book/docs/module-2/chapter-7"},"next":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-9"}},{"id":"module-3/chapter-10","title":"Advanced Humanoid Simulation in Isaac Sim","description":"Building upon the introduction to NVIDIA Isaac Sim, this chapter delves into the advanced techniques for simulating humanoid robots within its high-fidelity environment. We will explore creating complex robot models, leveraging realistic physics, and simulating dynamic interactions essential for developing sophisticated humanoid behaviors.","source":"@site/docs/module-3/chapter-10.md","sourceDirName":"module-3","slug":"/module-3/chapter-10","permalink":"/physical-ai-book/docs/module-3/chapter-10","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Advanced Humanoid Simulation in Isaac Sim","sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-9"},"next":{"title":"Integrating ROS 2 with NVIDIA Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-11"}},{"id":"module-3/chapter-11","title":"Integrating ROS 2 with NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a powerful simulation platform, but its true potential in robotics is unlocked through its seamless integration with ROS 2. This integration allows developers to leverage their existing ROS 2 tools, nodes, and workflows within the high-fidelity simulation environment provided by Isaac Sim, bridging the gap between simulation and real-world deployment.","source":"@site/docs/module-3/chapter-11.md","sourceDirName":"module-3","slug":"/module-3/chapter-11","permalink":"/physical-ai-book/docs/module-3/chapter-11","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Integrating ROS 2 with NVIDIA Isaac Sim","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Humanoid Simulation in Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-10"},"next":{"title":"Implementing Voice Control with Whisper","permalink":"/physical-ai-book/docs/module-4/chapter-13"}},{"id":"module-3/chapter-9","title":"Introduction to NVIDIA Isaac Sim","description":"As robotic systems become more complex and AI integration becomes standard, the need for highly realistic and scalable simulation platforms grows. NVIDIA Isaac Sim, built on the Omniverse platform, is a powerful, extensible, and photorealistic simulator designed to accelerate robotics development, testing, and AI training.","source":"@site/docs/module-3/chapter-9.md","sourceDirName":"module-3","slug":"/module-3/chapter-9","permalink":"/physical-ai-book/docs/module-3/chapter-9","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Introduction to NVIDIA Isaac Sim","sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Simulating Robot Sensors in Gazebo","permalink":"/physical-ai-book/docs/module-2/chapter-8"},"next":{"title":"Advanced Humanoid Simulation in Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-10"}},{"id":"module-4/chapter-13","title":"Implementing Voice Control with Whisper","description":"Enabling robots to understand and respond to natural human speech is a significant step towards more intuitive and accessible human-robot interaction (HRI). Voice control allows users to command robots using natural language, freeing them from complex interfaces. OpenAI's Whisper is a state-of-the-art, open-source Automatic Speech Recognition (ASR) system that excels at transcribing spoken language into text, even in challenging conditions.","source":"@site/docs/module-4/chapter-13.md","sourceDirName":"module-4","slug":"/module-4/chapter-13","permalink":"/physical-ai-book/docs/module-4/chapter-13","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"title":"Implementing Voice Control with Whisper","sidebar_position":13},"sidebar":"tutorialSidebar","previous":{"title":"Integrating ROS 2 with NVIDIA Isaac Sim","permalink":"/physical-ai-book/docs/module-3/chapter-11"},"next":{"title":"Task Planning and Navigation with LLMs and Nav2","permalink":"/physical-ai-book/docs/module-4/chapter-14"}},{"id":"module-4/chapter-14","title":"Task Planning and Navigation with LLMs and Nav2","description":"To make humanoid robots truly useful, they must be able to understand high-level goals and autonomously navigate their environment to achieve them. This chapter explores how Large Language Models (LLMs) can be used for intelligent task planning, and how the ROS 2 Navigation2 (Nav2) stack enables sophisticated robot navigation.","source":"@site/docs/module-4/chapter-14.md","sourceDirName":"module-4","slug":"/module-4/chapter-14","permalink":"/physical-ai-book/docs/module-4/chapter-14","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"title":"Task Planning and Navigation with LLMs and Nav2","sidebar_position":14},"sidebar":"tutorialSidebar","previous":{"title":"Implementing Voice Control with Whisper","permalink":"/physical-ai-book/docs/module-4/chapter-13"}},{"id":"Module1_IntroPhysicalAI","title":"Introduction to Physical AI","description":"The field of artificial intelligence has made tremendous strides in recent years, moving beyond purely digital realms to interact with and manipulate the physical world. This burgeoning area, often termed \"Physical AI\" or \"Embodied AI,\" is fundamentally about creating intelligent systems that can perceive, reason, plan, and act within the physical environment. Humanoid robots, with their potential to emulate human capabilities, stand at the forefront of this revolution. This chapter lays the groundwork for understanding Physical AI, its core concepts, and its critical role in developing intelligent embodied agents.","source":"@site/docs/Module1_IntroPhysicalAI.md","sourceDirName":".","slug":"/Module1_IntroPhysicalAI","permalink":"/physical-ai-book/docs/Module1_IntroPhysicalAI","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Physical AI","sidebar_position":1}},{"id":"Module1_ROSArchitecture","title":"ROS 2 Architecture","description":"Understanding the underlying architecture of ROS 2 is crucial for designing, developing, and debugging complex robotic systems. ROS 2 is designed to be a flexible, distributed middleware that facilitates communication between various software components (nodes) running on potentially different machines. This chapter will explore the core components of the ROS 2 architecture and how they enable powerful robotic applications.","source":"@site/docs/Module1_ROSArchitecture.md","sourceDirName":".","slug":"/Module1_ROSArchitecture","permalink":"/physical-ai-book/docs/Module1_ROSArchitecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"ROS 2 Architecture","sidebar_position":3}},{"id":"Module1_ROSComm","title":"ROS 2 Communication Patterns and Best Practices","description":"Effective communication between the various software components (nodes) of a robotic system is paramount for its correct functioning. ROS 2 provides a robust and flexible middleware built on DDS (Data Distribution Service) that offers several communication paradigms: Topics, Services, and Actions. Each pattern is designed for specific types of interaction, and understanding when to use which, along with adhering to best practices, is key to building reliable and efficient robotic applications.","source":"@site/docs/Module1_ROSComm.md","sourceDirName":".","slug":"/Module1_ROSComm","permalink":"/physical-ai-book/docs/Module1_ROSComm","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"ROS 2 Communication Patterns and Best Practices","sidebar_position":5}},{"id":"Module1_ROSPackages","title":"ROS 2 Packages and Workspaces","description":"In ROS 2, the fundamental unit of code organization, distribution, and management is the package. Packages encapsulate related software components, configurations, and resources, allowing for modular development and easy integration into larger robotic systems. Understanding how to create, manage, and build these packages, along with the concept of workspaces, is essential for any ROS 2 developer. This chapter will guide you through these core concepts.","source":"@site/docs/Module1_ROSPackages.md","sourceDirName":".","slug":"/Module1_ROSPackages","permalink":"/physical-ai-book/docs/Module1_ROSPackages","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"ROS 2 Packages and Workspaces","sidebar_position":4}},{"id":"Module1_SensorSystems","title":"Sensor Systems","description":"To perceive and interact with the physical world, any intelligent agent, especially a robot, must be equipped with sensing capabilities. Sensors are the robot's \"senses,\" providing the raw data from which it builds an understanding of its own state and its surrounding environment. This chapter explores the fundamental types of sensor systems critical for robotics, their principles of operation, and how they are represented and utilized within the ROS 2 framework.","source":"@site/docs/Module1_SensorSystems.md","sourceDirName":".","slug":"/Module1_SensorSystems","permalink":"/physical-ai-book/docs/Module1_SensorSystems","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Sensor Systems","sidebar_position":2}},{"id":"Module2_DigitalTwin","title":"Introduction to Digital Twins","description":"In the realm of robotics and AI, creating intelligent agents that can operate effectively in the physical world is paramount. A critical tool that bridges the gap between the virtual and physical domains is the Digital Twin. A Digital Twin is a dynamic, virtual representation of a physical asset, process, or system, constantly updated with real-time data. This chapter introduces the concept of Digital Twins, their components, and their profound implications for robotics development, simulation, and operational intelligence.","source":"@site/docs/Module2_DigitalTwin.md","sourceDirName":".","slug":"/Module2_DigitalTwin","permalink":"/physical-ai-book/docs/Module2_DigitalTwin","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Digital Twins","sidebar_position":1}},{"id":"Module3_NVIDIAIsaac","title":"Introduction to NVIDIA Isaac Sim","description":"In the journey to create intelligent, physically embodied robots, simulation is not merely a helpful tool; it is an indispensable cornerstone. As robotic systems grow more complex and AI integration becomes deeper, the need for high-fidelity, scalable, and versatile simulation environments escalates. NVIDIA Isaac Sim, built upon the powerful NVIDIA Omniverse platform, stands as a leading solution, accelerating robotic development, AI training, and the creation of sophisticated digital twins. This chapter introduces Isaac Sim, exploring its capabilities, advantages, and its role in the modern robotics development ecosystem.","source":"@site/docs/Module3_NVIDIAIsaac.md","sourceDirName":".","slug":"/Module3_NVIDIAIsaac","permalink":"/physical-ai-book/docs/Module3_NVIDIAIsaac","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to NVIDIA Isaac Sim","sidebar_position":1}},{"id":"Module4_ActionGeneration","title":"Action Generation and Control in Embodied AI","description":"The ultimate goal of Physical AI and VLA systems is to enable robots to act effectively in the physical world. Action generation is the process by which a robot translates high-level goals, perceptions, and linguistic commands into a sequence of physical movements and interactions. This chapter delves into the principles and techniques behind generating and controlling robot actions, focusing on how embodied AI systems can achieve goal-directed behavior.","source":"@site/docs/Module4_ActionGeneration.md","sourceDirName":".","slug":"/Module4_ActionGeneration","permalink":"/physical-ai-book/docs/Module4_ActionGeneration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Action Generation and Control in Embodied AI","sidebar_position":4}},{"id":"Module4_Challenges","title":"Challenges, Limitations, and Ethical Considerations","description":"As Vision–Language–Action (VLA) systems advance and become more integrated into robotics, it is crucial to critically examine their challenges, inherent limitations, and the profound ethical implications they present. Moving from laboratory demonstrations to real-world deployment requires addressing issues of reliability, safety, interpretability, and societal impact. This chapter explores these critical considerations.","source":"@site/docs/Module4_Challenges.md","sourceDirName":".","slug":"/Module4_Challenges","permalink":"/physical-ai-book/docs/Module4_Challenges","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Challenges, Limitations, and Ethical Considerations","sidebar_position":8}},{"id":"Module4_FutureDirections","title":"Future Directions of Vision–Language–Action Systems","description":"The field of Vision–Language–Action (VLA) systems is rapidly evolving, pushing the boundaries of what intelligent embodied agents can achieve. As current systems become more capable, researchers are exploring new frontiers that promise even more sophisticated robots. This chapter looks ahead, discussing the exciting future directions for VLA systems, anticipating advancements in their capabilities, architectures, and applications.","source":"@site/docs/Module4_FutureDirections.md","sourceDirName":".","slug":"/Module4_FutureDirections","permalink":"/physical-ai-book/docs/Module4_FutureDirections","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Future Directions of Vision–Language–Action Systems","sidebar_position":9}},{"id":"Module4_LanguageUnderstanding","title":"Language Understanding and Large Language Models in Robotics","description":"The ability for robots to understand and respond to natural human language is a cornerstone of intuitive human-robot interaction (HRI) and advanced task execution. This chapter explores the domain of language understanding in robotics, with a particular focus on the transformative impact of Large Language Models (LLMs). We will cover how robots can interpret commands, engage in dialogue, and leverage LLMs for complex reasoning and planning tasks.","source":"@site/docs/Module4_LanguageUnderstanding.md","sourceDirName":".","slug":"/Module4_LanguageUnderstanding","permalink":"/physical-ai-book/docs/Module4_LanguageUnderstanding","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Language Understanding and Large Language Models in Robotics","sidebar_position":3}},{"id":"Module4_MultimodalFusion","title":"Multimodal Fusion: Connecting Vision, Language, and Action","description":"The true power of VLA systems lies not just in processing vision, language, or action independently, but in their ability to fuse these modalities. Multimodal fusion allows a robot to build a richer, more contextual understanding of its environment and tasks by integrating information from different sensory and cognitive streams. This chapter explores the importance and techniques of multimodal fusion, enabling robots to connect what they see, what they hear or read, and what they need to do.","source":"@site/docs/Module4_MultimodalFusion.md","sourceDirName":".","slug":"/Module4_MultimodalFusion","permalink":"/physical-ai-book/docs/Module4_MultimodalFusion","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Multimodal Fusion: Connecting Vision, Language, and Action","sidebar_position":5}},{"id":"Module4_TaskNavLLM","title":"Task Planning and Navigation with LLMs and Nav2","description":"To imbue humanoid robots with true autonomy, they must be capable of understanding high-level goals and executing complex sequences of actions in their environment. This requires a sophisticated integration of artificial intelligence for task decomposition and robust navigation systems for movement. This chapter explores how Large Language Models (LLMs) can be leveraged for intelligent task planning and how ROS 2's Navigation2 (Nav2) stack provides powerful capabilities for robot navigation, demonstrating how these components work together to enable advanced robot autonomy.","source":"@site/docs/Module4_TaskNavLLM.md","sourceDirName":".","slug":"/Module4_TaskNavLLM","permalink":"/physical-ai-book/docs/Module4_TaskNavLLM","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Task Planning and Navigation with LLMs and Nav2","sidebar_position":2}},{"id":"Module4_UseCases","title":"Real-World Robotics Use Cases","description":"The integration of vision, language, and action capabilities is transforming robots from specialized tools into versatile agents capable of performing complex tasks in human-centric environments. This chapter explores practical, real-world applications where VLA systems are making a significant impact, covering advancements in navigation, manipulation, and human-robot interaction, and illustrating how these technologies are moving from research labs into our daily lives.","source":"@site/docs/Module4_UseCases.md","sourceDirName":".","slug":"/Module4_UseCases","permalink":"/physical-ai-book/docs/Module4_UseCases","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Real-World Robotics Use Cases","sidebar_position":7}},{"id":"Module4_VisualPerception","title":"Visual Perception for Robotics","description":"Vision is arguably the most crucial sense for robots operating in complex, unstructured environments. It allows them to perceive their surroundings, identify objects, understand spatial relationships, and navigate safely. This chapter delves into the principles of visual perception in robotics, covering various camera types, the processing of 2D and 3D visual data, and the concept of scene understanding, all essential for equipping robots with sight.","source":"@site/docs/Module4_VisualPerception.md","sourceDirName":".","slug":"/Module4_VisualPerception","permalink":"/physical-ai-book/docs/Module4_VisualPerception","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Visual Perception for Robotics","sidebar_position":2}},{"id":"Module4_VLA_Intro","title":"Introduction to Vision–Language–Action (VLA) Systems","description":"The frontier of artificial intelligence is increasingly focused on creating agents that can understand and interact with the world in ways that are more intuitive and human-like. A key aspect of this endeavor is the development of Vision–Language–Action (VLA) systems. These systems aim to bridge the gap between perceiving the world visually, understanding human instructions expressed in natural language, and generating appropriate physical actions to fulfill those instructions. This chapter introduces VLA systems, explaining their core concepts, the challenges they address, and their transformative potential in robotics and embodied AI.","source":"@site/docs/Module4_VLA_Intro.md","sourceDirName":".","slug":"/Module4_VLA_Intro","permalink":"/physical-ai-book/docs/Module4_VLA_Intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Vision–Language–Action (VLA) Systems","sidebar_position":1}},{"id":"Module4_VLAPipelines","title":"Vision–Language–Action Architectures and Pipelines","description":"The effective integration of visual perception, language understanding, and action generation is at the core of building intelligent embodied agents. This chapter explores the various architectural patterns and pipelines that enable Vision–Language–Action (VLA) systems, discussing how different components are connected and how data flows from raw sensory input to physical action. Understanding these architectures is key to designing sophisticated robots capable of complex, context-aware tasks.","source":"@site/docs/Module4_VLAPipelines.md","sourceDirName":".","slug":"/Module4_VLAPipelines","permalink":"/physical-ai-book/docs/Module4_VLAPipelines","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Vision–Language–Action Architectures and Pipelines","sidebar_position":6}},{"id":"Module4_VoiceControl","title":"Implementing Voice Control with Whisper","description":"Enabling robots to understand and respond to natural human speech is a significant step towards more intuitive and accessible human-robot interaction (HRI). Voice control allows users to command robots using natural language, freeing them from complex interfaces. OpenAI's Whisper is a state-of-the-art, open-source Automatic Speech Recognition (ASR) system that excels at transcribing spoken language into text, even in challenging conditions. This chapter details how to leverage Whisper for robotic voice control and integrate it into a ROS 2 system.","source":"@site/docs/Module4_VoiceControl.md","sourceDirName":".","slug":"/Module4_VoiceControl","permalink":"/physical-ai-book/docs/Module4_VoiceControl","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Implementing Voice Control with Whisper","sidebar_position":1}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"Module 1: ROS 2 Fundamentals","collapsed":false,"items":[{"type":"doc","id":"module-1/chapter-1"},{"type":"doc","id":"module-1/chapter-2"},{"type":"doc","id":"module-1/chapter-3"},{"type":"doc","id":"module-1/chapter-4"},{"type":"doc","id":"module-1/chapter-5"}],"collapsible":true},{"type":"category","label":"Module 2: Digital Twin","collapsed":false,"items":[{"type":"doc","id":"module-2/chapter-6"},{"type":"doc","id":"module-2/chapter-7"},{"type":"doc","id":"module-2/chapter-8"}],"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac","collapsed":false,"items":[{"type":"doc","id":"module-3/chapter-9"},{"type":"doc","id":"module-3/chapter-10"},{"type":"doc","id":"module-3/chapter-11"}],"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action","collapsed":false,"items":[{"type":"doc","id":"module-4/chapter-13"},{"type":"doc","id":"module-4/chapter-14"}],"collapsible":true}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/physical-ai-book/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/physical-ai-book/","source":"@site/src/pages/index.tsx"},{"type":"mdx","permalink":"/physical-ai-book/markdown-page","source":"@site/src/pages/markdown-page.md","title":"Markdown page example","description":"You don't need React to write simple standalone pages.","frontMatter":{"title":"Markdown page example"},"unlisted":false}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-theme-mermaid":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}